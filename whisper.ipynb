{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import re\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import openai\n",
    "import edge_tts\n",
    "import asyncio\n",
    "import time\n",
    "import sounddevice as sd\n",
    "from speechbrain.inference.classifiers import EncoderClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for recording\n",
    "SAMPLE_RATE = 16000  # Sample rate for recording\n",
    "RECORD_SECONDS = 3  # Duration for recording in seconds\n",
    "\n",
    "# Define available language options for transcription, GPT, and TTS\n",
    "LANGUAGE_SETTINGS = {\n",
    "    'ur': {\n",
    "        'whisper_language': 'urdu',\n",
    "        'tts_voice': 'ur-PK-AsadNeural',\n",
    "        'gpt_language_code': 'ur',  # Urdu language code for GPT prompt\n",
    "    },\n",
    "    'en': {\n",
    "        'whisper_language': 'english',\n",
    "        'tts_voice': 'en-US-JennyNeural',\n",
    "        'gpt_language_code': 'en',  # English language code for GPT prompt\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the SpeechBrain language identification model\n",
    "language_id_model = EncoderClassifier.from_hparams(source=\"speechbrain/lang-id-voxlingua107-ecapa\", savedir=\"tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to detect language from audio using SpeechBrain\n",
    "def detect_language(audio_file_path):\n",
    "    \"\"\"Detects the language of the given audio file.\"\"\"\n",
    "    signal, sample_rate = torchaudio.load(audio_file_path)\n",
    "\n",
    "    # Ensure signal is in batch format (batch_size, num_samples)\n",
    "    if signal.dim() == 1:\n",
    "        signal = signal.unsqueeze(0)\n",
    "\n",
    "    # Use SpeechBrain model to detect language\n",
    "    prediction = language_id_model.classify_batch(signal)\n",
    "\n",
    "    # Extract language code (prediction[3] gives the predicted language label)\n",
    "    language_code = prediction[3][0].split(\":\")[0]  # Get the language code (e.g., 'ur', 'en')\n",
    "    language_name = prediction[3][0].split(\":\")[1].strip()  # Get the language name (e.g., 'Urdu', 'English')\n",
    "\n",
    "    print(f\"Detected language: {language_name} ({language_code})\")\n",
    "    return language_code\n",
    "\n",
    "# Function to select language (switches to Urdu if detected language is not English)\n",
    "def select_language(language_code):\n",
    "    \"\"\"Selects language settings for Whisper, GPT, and Edge TTS based on detected language code.\"\"\"\n",
    "    if language_code != 'en':\n",
    "        return LANGUAGE_SETTINGS['ur']  # Switch to Urdu for any non-English language\n",
    "    return LANGUAGE_SETTINGS['en']  # Default to English\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans the input text by removing unwanted characters.\"\"\"\n",
    "    try:\n",
    "        text.encode('utf-8').decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        text = text.encode('utf-8', 'replace').decode('utf-8')\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text.strip()  # Strip leading/trailing whitespace\n",
    "\n",
    "def transcribe_audio(audio_file, whisper_language):\n",
    "    \"\"\"Transcribes audio using the Whisper model.\"\"\"\n",
    "    print(\"Transcribing audio using Whisper model...\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "    # Resample to 16kHz if necessary\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "\n",
    "    # Process the audio with Whisper\n",
    "    forced_decoder_ids = processor.get_decoder_prompt_ids(language=whisper_language, task=\"transcribe\")\n",
    "    input_features = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    print(f\"Transcribed Text: {transcription}\")\n",
    "    return transcription\n",
    "\n",
    "def generate_gpt_response(transcribed_text, language_code):\n",
    "    \"\"\"Generate a response based on the detected language.\"\"\"\n",
    "    if language_code == 'ur':\n",
    "        prompt = (\n",
    "            \"آپ ایک مددگار اسسٹنٹ ہیں۔ براہ کرم مندرجہ ذیل معلومات پر ایک واضح اور جامع جواب فراہم کریں: \\n\\n\"\n",
    "            f\"Input: {transcribed_text}\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"You are a helpful assistant. Based on the following input, please provide a clear and concise response: \\n\\n\"\n",
    "            f\"Input: {transcribed_text}\"\n",
    "        )\n",
    "    \n",
    "    # Generate GPT response using the custom prompt\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",  # Use the appropriate model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    # Extract the generated response\n",
    "    gpt_response = response['choices'][0]['message']['content'].strip()\n",
    "    print(\"gpt_response\", gpt_response)\n",
    "    return gpt_response\n",
    "\n",
    "async def edge_tts_speech(text, voice, output_file='response-brain-sp.mp3'):\n",
    "    \"\"\"Convert text to speech using Edge TTS and save it as an MP3 file.\"\"\"\n",
    "    print(\"Converting text to speech using Edge TTS...\")\n",
    "    communicate = edge_tts.Communicate(text, voice)\n",
    "    await communicate.save(output_file)\n",
    "    print(f\"Audio file saved as {output_file}\")\n",
    "\n",
    "# Function to record audio\n",
    "def record_audio(filename='output.wav', duration=RECORD_SECONDS, sample_rate=SAMPLE_RATE):\n",
    "    \"\"\"Record audio for a specified duration and save it to a file.\"\"\"\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    # Save the recorded data to a WAV file\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)  # Mono\n",
    "        wf.setsampwidth(2)  # 16-bit audio\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes((audio_data * 32767).astype(np.int16).tobytes())  # Convert float to int16\n",
    "    \n",
    "    return filename  # Return the filename for later use\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to detect language, transcribe, respond, and synthesize speech.\"\"\"\n",
    "    # Record the audio\n",
    "    audio_file_path = record_audio()\n",
    "\n",
    "    # Detect language from the recorded audio file\n",
    "    de = time.time()\n",
    "    detected_language_code = detect_language(audio_file_path)\n",
    "    d2 = time.time()\n",
    "    print(\"Language detection took:\", d2 - de)\n",
    "\n",
    "    # Get language-specific settings based on detected language\n",
    "    language_settings = select_language(detected_language_code)\n",
    "    whisper_language = language_settings['whisper_language']\n",
    "    tts_voice = language_settings['tts_voice']\n",
    "    gpt_language_code = language_settings['gpt_language_code']\n",
    "\n",
    "    # Transcribe the audio using Whisper\n",
    "    t3 = time.time()\n",
    "    transcribed_text = transcribe_audio(audio_file_path, whisper_language)\n",
    "    t4 = time.time()\n",
    "    print(\"Transcription took:\", t4 - t3)\n",
    "\n",
    "    # Generate GPT response with language-based prompt\n",
    "    t5 = time.time()\n",
    "    gpt_response = generate_gpt_response(transcribed_text, gpt_language_code)\n",
    "    t6 = time.time()\n",
    "    print(\"GPT response generation took:\", t6 - t5)\n",
    "\n",
    "    # Synthesize speech asynchronously using the selected TTS voice\n",
    "    t7 = time.time()\n",
    "    asyncio.create_task(edge_tts_speech(gpt_response, tts_voice))\n",
    "    t8 = time.time()\n",
    "    print(\"TTS generation took:\", t8 - t7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
